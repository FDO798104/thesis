# settings.ini
# All paths and settings must be correct for the code to work. 
[main]

# Place here the full path of the URL dataset.csv file
csv_dataset_location = C:/Users/USER/SCRAPER/dataset.csv

# Place here the full path of the JSON credentials file necessary to upload data to Google Cloud
json_credentials_path = C:/Users/USER/SCRAPER/CREDENTIALS/sage-ripple-KEY.json

# Place here the fill path of where you wish local back-up of scraped data should be stored.
local_storage_path = C:/Users/USER/SCRAPER/

# Number of threads for parallel scraping. 1 = sequential. The default value is 10, do not use more than 10 threads. If the average processing speed is consistently larger than 10 sites/second lower the number of threads!
threads = 5

# The scraping is done in batches. The results of each batch are stored in a separate .xlsx file.
# e.g. if batch size is 5000 60+ files will be creates. If the batch size is 50000 6+ files will be created.
batch_size = 10000

# In case the process gets interrupted fill in which batch was last fully completed by looking at the last created textdata_{index}.xlsx file. Fill in the index here and restart the code. Keep the batch_size unchanged!!
# dafualt value is -1!
last_completed_batch = -1

# If you want to save in .parquet format (avg file size 3 times larger than .xlsx, smaller batch_size might be necessary). Set to True for .parquet and set to False for .xlsx (not recommended). 
parquet = True

# Timeout parameters for uploading to Google Cloud. connect_time_out is the maximum time for establishing a connection (default = 10).
# upload_time_out sets the maximum time for uploading a file. This is a tricky setting. Leaving it to the Google Cloud default setting will cause larger files to time out. Setting the value to high and a genuine problem with the connection will slow down the code and the upload will still fail. (300 should be more than enough for all files if batch_size <= 10.000)   
connect_time_out = 10
upload_time_out = 300