{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-3vrqtIHE51"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "import os\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.run([\"pip\", \"install\", \"spacy\"])\n",
        "subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"nl_core_news_sm\"])\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "5o36IpEaNUWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmVR8tvt94k7"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  # Does the loaded textdata contain scraped HTML code? --> set to True.\n",
        "  extract_text_from_html = False\n",
        "  tags_to_extract = ['p','h','li','span','td']\n",
        "  backup_files = True\n",
        "\n",
        "  # Set to True if you want to handle duplicates in any way. If set to False than\n",
        "  # two settings below will be ignored.\n",
        "  handle_duplicates = False\n",
        "  # Set use_duplicate_removal_heuristic to True for heuristic\n",
        "  use_duplicate_removal_heuristic = False\n",
        "  # Set to True if you want to delete all duplicates instead of using heuristic\n",
        "  remove_all_duplicates = True\n",
        "\n",
        "  # Set to True if you want to remove rows with to few words. If use_duplicate_removal_heurstic is used than all rows with less than 10 words will already be removed.\n",
        "  do_remove_rows_with_to_few_words = False\n",
        "\n",
        "  # Set to number of words you want a row to have minimum\n",
        "  min_words = 20\n",
        "\n",
        "  # Downcase the text\n",
        "  do_downcasing = False\n",
        "\n",
        "  # Remove all numbers from text\n",
        "  do_remove_numbers = False\n",
        "\n",
        "  # Remove puntuation from text\n",
        "  do_remove_punctuation = False\n",
        "\n",
        "  # Remove stopwords\n",
        "  do_remove_stopwords = False\n",
        "\n",
        "  # Remove set of words related to cookies message or javascript not activated banner\n",
        "  do_remove_cookies_etc_message = False\n",
        "  do_remove_java = False\n",
        "\n",
        "  # Remove single characters and extra white spaces\n",
        "  do_remove_noise = True\n",
        "\n",
        "  # Remove the rows where no NACE code is linked with the DATA\n",
        "  # !! Can't think of a single reason to set this to False !!\n",
        "  do_remove_null_nace = False\n",
        "\n",
        "  # Create ngrams after pre processing\n",
        "  create_ngrams = True\n",
        "\n",
        "  # Set range for ngrams\n",
        "  ngram_range = (3,6)\n",
        "\n",
        "  # Use Snowball Stemmer\n",
        "  use_snowball_stemmer = False\n",
        "\n",
        "  # Use lemmatization\n",
        "  lemmatize_text = False\n",
        "\n",
        "  run_pre_processing(extract_text_from_html,\n",
        "                    tags_to_extract,\n",
        "                    handle_duplicates,\n",
        "                    use_duplicate_removal_heuristic,\n",
        "                    remove_all_duplicates,\n",
        "                    do_remove_rows_with_to_few_words,\n",
        "                    min_words,\n",
        "                    do_downcasing,\n",
        "                    do_remove_numbers,\n",
        "                    do_remove_punctuation,\n",
        "                    do_remove_stopwords,\n",
        "                    do_remove_cookies_etc_message,\n",
        "                    do_remove_java,\n",
        "                    do_remove_noise,\n",
        "                    do_remove_null_nace,\n",
        "                    create_ngrams,\n",
        "                    ngram_range,\n",
        "                    use_snowball_stemmer,\n",
        "                    lemmatize_text,\n",
        "                    backup_files)\n",
        "\n",
        "def run_pre_processing(do_extract_text_from_html,\n",
        "                       tags_to_extract,\n",
        "                       handle_duplicates,\n",
        "                       use_duplicate_removal_heuristic,\n",
        "                       remove_all_duplicates,\n",
        "                       do_remove_rows_with_to_few_words,\n",
        "                       min_words,\n",
        "                       do_downcasing,\n",
        "                       do_remove_numbers,\n",
        "                       do_remove_punctuation,\n",
        "                       do_remove_stopwords,\n",
        "                       do_remove_cookies_etc_message,\n",
        "                       do_remove_java,\n",
        "                       do_remove_noise,\n",
        "                       do_remove_null_nace,\n",
        "                       create_ngrams,\n",
        "                       ngram_range,\n",
        "                       use_snowball_stemmer,\n",
        "                       lemmatize_text,\n",
        "                       backup_files):\n",
        "  warning = False\n",
        "\n",
        "  if not os.path.isdir(files_directory):\n",
        "    raise FileNotFoundError(f\"The value in original_files_directory is not an existing directory \\n {files_directory}\")\n",
        "\n",
        "  if not os.path.isdir(files_directory):\n",
        "    os.mkdir(files_directory)\n",
        "\n",
        "  elif not os.path.isdir(backup_files_directory):\n",
        "    os.mkdir(backup_files_directory)\n",
        "\n",
        "\n",
        "  conflict_1 = sum([use_duplicate_removal_heuristic, remove_all_duplicates])\n",
        "  conflict_2 = sum([create_ngrams, use_snowball_stemmer, lemmatize_text])\n",
        "\n",
        "  if handle_duplicates and conflict_1 > 1:\n",
        "    raise ValueError(\"There is a conflict in the settings. \\'use_duplicate_removal_heuristic\\' is set to \\'True\\' and \\'remove_all_duplicates\\' is set to \\'True\\'. \\n \\\n",
        "    If you want to use duplicate removal heuristic set \\'delete_all_duplicates\\' to \\'False\\'\\n\")\n",
        "\n",
        "  if handle_duplicates and conflict_1 == 0:\n",
        "    raise ValueError(\"There is a conflict in the settings. \\'use_duplicate_removal_heuristic\\' is set to \\'False\\' and \\'remove_all_duplicates\\' is set to \\'False\\'. \\n \\\n",
        "    \\'handle_duplicates\\' is set to \\'True\\'. One handling method must be chosen or \\'handle_duplicates\\' must be set to \\'False\\'.\\n\")\n",
        "    warning = True\n",
        "\n",
        "  if conflict_2 > 1:\n",
        "    raise ValueError(f\"There is a conflict in the settings. Of the following settings only 1 can be set to \\'True\\': \\n \\\n",
        "    \\'create_ngrams\\' is set to \\'{create_ngrams}\\' \\n \\\n",
        "    \\'snowball_stemmer\\' is set to \\'{use_snowball_stemmer}\\' \\n \\\n",
        "    \\'lemmatize_text\\' is set to \\'{lemmatize_text}\\' \\n \\\n",
        "    Correct this mistake before proceeding!\")\n",
        "\n",
        "  if not do_remove_null_nace:\n",
        "    print(f'WARNING - The \\'do_remove_null_nace\\' setting is set to \\'{do_remove_null_nace}\\'.\\n\\\n",
        "    Make sure that this is correct as not removing NULL values in the NACE columns will cause errors when training a model. \\n')\n",
        "    warning = True\n",
        "\n",
        "  if do_remove_java and not do_remove_cookies_etc_message:\n",
        "    print(f\"If /'do_remove_cookies_etc_message/' is set to /'False/' the /'do_remove_jave/' setting will be ignored\")\n",
        "    warning = True\n",
        "\n",
        "  if warning:\n",
        "    print('\\n \\n')\n",
        "    input_answer = input(f\"WARNING: A deviation from the recommended settings was detected. Read the warnings above. Do you wish to ignore these warnings and continue? (yes/no)\")\n",
        "    if input_answer.lower() != 'yes':\n",
        "      raise RuntimeError(\"Execution halted due to warnings.\")\n",
        "    else:\n",
        "      print(\"Continuing despite warnings\")\n",
        "\n",
        "  if backup_files:\n",
        "    for file_name, file_path in get_files().items():\n",
        "        data = read_file(file_path)\n",
        "        store_file(data, file_name, backup=True)\n",
        "\n",
        "  input_answer = input(\"Directories reset to values provided in settings. If you have already done part of the pre processing make sure you select the directory to the partly pre processed files! Do you wish to continue? (yes/no)\")\n",
        "  if input_answer.lower() != 'yes':\n",
        "    raise RuntimeError(\"Execution halted due to warnings.\")\n",
        "  elif warning:\n",
        "    print(\"Continuing despite warnings\")\n",
        "\n",
        "  if do_extract_text_from_html:\n",
        "    print(\"Extracting texts...\")\n",
        "    extract_text_from_html(tags_to_extract)\n",
        "    print(\"Extracting texts complete\")\n",
        "\n",
        "  if handle_duplicates:\n",
        "    print(f'Handling duplicates')\n",
        "    duplicates_handler(use_duplicate_removal_heuristic, remove_all_duplicates)\n",
        "\n",
        "  if do_downcasing:\n",
        "    print(\"Reading files to downcase text...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      data = read_file(file_path)\n",
        "      data['Text'] = data['Text'].apply(downcase_text)\n",
        "\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\nDOWNCASING text completed\\n\")\n",
        "\n",
        "  if do_remove_numbers:\n",
        "    print(\"Reading files to remove numbers...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      data = read_file(file_path)\n",
        "      data['Text'] = data['Text'].apply(remove_numbers)\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\REMOVING numbers in text completed\\n\")\n",
        "\n",
        "  if do_remove_punctuation:\n",
        "    print(\"Reading files to remove punctuation...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      data = read_file(file_path)\n",
        "      data['Text'] = data['Text'].apply(remove_punctuations)\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\REMOVING punctuation in text completed\\n\")\n",
        "\n",
        "  if do_remove_cookies_etc_message:\n",
        "    print(\"Reading files to remove copyright and cookies message...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      data = read_file(file_path)\n",
        "      data['Text'] = data['Text'].apply(remove_cookies_copyright)\n",
        "      if do_remove_java:\n",
        "        print(\"Removing JAVA message\")\n",
        "        data['Text'] = data['Text'].apply(remove_javascript)\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\Removing copyright and cookies in text completed \\n\")\n",
        "\n",
        "  if do_remove_noise:\n",
        "    print(\"Reading files to remove noise...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      data = read_file(file_path)\n",
        "      data['Text'] = data['Text'].apply(remove_noise)\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\REMOVING noise in text completed\\n\")\n",
        "\n",
        "  if do_remove_null_nace:\n",
        "    print(\"Reading files to remove null nace rows...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      data = read_file(file_path)\n",
        "      data = remove_null_nace_rows(data)\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\REMOVING null nace rows in text completed\\n\")\n",
        "\n",
        "\n",
        "  if do_remove_rows_with_to_few_words:\n",
        "    count = 0\n",
        "    print(\"Reading files to remove rows with to few words...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      data = read_file(file_path)\n",
        "      data, count = count_words(data, count, min_words)\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\REMOVING rows with to few words completed\\n\")\n",
        "\n",
        "  #Remove stopwords from text #DONE\n",
        "  def remove_stopwords(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word.lower() not in stopwords_dutch]\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "  if do_remove_stopwords:\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')\n",
        "    stopwords_dutch = set(stopwords.words('dutch'))\n",
        "    print(\"Reading files to remove stopwords in text...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      data = read_file(file_path)\n",
        "      data['Text'] = data['Text'].apply(remove_stopwords)\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\REMOVING stopwords from text completed\\n\")\n",
        "\n",
        "  if create_ngrams:\n",
        "    print(\"Reading files to create ngrams...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      print(f'processing {file_name}')\n",
        "      data = read_file(file_path)\n",
        "      data['Text'] = data['Text'].apply(tokenize_ngrams)\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\Creating ngrams text completed\\n\")\n",
        "\n",
        "  if lemmatize_text:\n",
        "    nlp = spacy.load(\"nl_core_news_sm\")\n",
        "    print(\"Reading files to lemmatize...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      print(f'processing {file_name}')\n",
        "      data = read_file(file_path)\n",
        "      data = lemmatize_dutch_text(data, nlp, 'Text')\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\Lemmatizing text completed\\n\")\n",
        "\n",
        "  if use_snowball_stemmer:\n",
        "    stemmer = SnowballStemmer(\"dutch\")\n",
        "    print(\"Reading files to stem text...\")\n",
        "    for file_name, file_path in get_files().items():\n",
        "      print(f'processing {file_name}')\n",
        "      data = read_file(file_path)\n",
        "      data = stem_dutch_text(data, stemmer, 'Text')\n",
        "      store_file(data, file_name, backup=False)\n",
        "      del data\n",
        "    print(\"\\Stemming text completed\\n\")\n",
        "\n",
        "print(\"Process finished\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "csV0mo2EkVk2"
      },
      "outputs": [],
      "source": [
        "def get_files(overwrite = False, new_path = None):\n",
        "  files_dict = dict()\n",
        "  files_directory_temp = files_directory\n",
        "  if overwrite and new_path != None:\n",
        "    files_directory_temp = new_path\n",
        "\n",
        "  for file_name in os.listdir(files_directory_temp):\n",
        "    if file_name.endswith('.parquet'):\n",
        "      file_path = os.path.join(files_directory_temp, file_name)\n",
        "      files_dict[file_name] = file_path\n",
        "  return files_dict\n",
        "\n",
        "# This function reads a file path and returns the dataframe\n",
        "def read_file(file_path, all=True):\n",
        "  print(\"Processing file:\", file_path)\n",
        "  if file_path.endswith('.parquet'):\n",
        "    data = pd.read_parquet(file_path)\n",
        "  return data\n",
        "\n",
        "def store_file(data, file_name, backup=False):\n",
        "  if backup:\n",
        "    directory_temp = backup_files_directory\n",
        "  else:\n",
        "    directory_temp = files_directory\n",
        "  save_path = os.path.join(directory_temp, file_name)\n",
        "  data.to_parquet(save_path)\n",
        "  print(\"File processed:\", save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AIMHe7ijHKZa"
      },
      "outputs": [],
      "source": [
        "# Extract text with certain tags #DONE\n",
        "def extract_text(html, tags_to_extract):\n",
        "  try:\n",
        "    del extracted_text\n",
        "  except:\n",
        "    None\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "  extracted_text = []\n",
        "  for tag in tags_to_extract:\n",
        "    extracted_text.extend([tag.get_text() for tag in soup.find_all(tag)])\n",
        "  return ' '.join(extracted_text)\n",
        "\n",
        "\n",
        "def extract_text_from_html(tags_to_extract):\n",
        "  print(\"Reading files to extract text from HTML...\")\n",
        "  for file_name, file_path in get_files().items():\n",
        "    data = read_file(file_path)\n",
        "    data['Text'] = data['Text'].apply(lambda x: extract_text(x, tags_to_extract))\n",
        "    store_file(data, file_name, backup=False)\n",
        "    del data\n",
        "  print(\"\\nEXTRACTING text completed\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apwzLti6FYaB"
      },
      "outputs": [],
      "source": [
        "def hash_and_collect(data, total):\n",
        "  data['Text_Hash'] = data['Text'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())\n",
        "  del data['Text']\n",
        "  total = pd.concat([total,data], ignore_index = True)\n",
        "  return total\n",
        "\n",
        "def remove_selected_kbo(kbo_removal_list):\n",
        "  remove_count = 0\n",
        "  for file_name, file_path in get_files().items():\n",
        "    data = read_file(file_path)\n",
        "    before = data.shape[0]\n",
        "    data = data[~data['KboNr'].isin(kbo_removal_list)]\n",
        "    store_file(data, file_name, backup=False)\n",
        "    after = data.shape[0]\n",
        "    remove_count += (before-after)\n",
        "    del data\n",
        "  print(\"\\nHANDLING DUPLICATES completed\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# duplicate_handler() should be called if duplicates need to be removed\n",
        "# First all data will be loaded, hashed and added to the same dataframe\n",
        "def duplicates_handler(use_duplicate_removal_heuristic, remove_all_duplicates):\n",
        "  # The kbo removal list will contain all the KBOs we want to delete\n",
        "  kbo_removal_list = []\n",
        "  hashed_total_data = pd.DataFrame()\n",
        "  # Load data, hash it and add to single dataframe\n",
        "  for file_name, file_path in get_files().items():\n",
        "    data = read_file(file_path)\n",
        "    hashed_total_data = hash_and_collect(data, hashed_total_data)\n",
        "    del data\n",
        "  # Count how often a hash appears\n",
        "  duplicate_counts = hashed_total_data['Text_Hash'].value_counts()\n",
        "  # Create hashed_dict containing the hashes as key and the associated KBO's in a list as value\n",
        "  hashes_dict = dict()\n",
        "  for index, row in hashed_total_data.iterrows():\n",
        "    current_hash = row['Text_Hash']\n",
        "    current_KBO = row['KboNr']\n",
        "    # Check if hash exists as key. If so add to existing list.\n",
        "    if current_hash in hashes_dict.keys():\n",
        "      hashes_dict[current_hash].append(current_KBO)\n",
        "    else:\n",
        "      hashes_dict[current_hash] = [current_KBO]\n",
        "  if use_duplicate_removal_heuristic:\n",
        "    kbo_removal_list = heuristic_removal(hashes_dict, kbo_removal_list)\n",
        "\n",
        "  elif remove_all_duplicates:\n",
        "    kbo_removal_list = remove_duplicates_complete(hashes_dict, kbo_removal_list)\n",
        "\n",
        "  else:\n",
        "    kbo_removal_list = remove_always(hashes_dict, kbo_removal_list)\n",
        "\n",
        "  remove_selected_kbo(kbo_removal_list)\n",
        "\n",
        "def remove_duplicates_complete(hashes_dict, kbo_removal_list):\n",
        "  for hash, kbo_list in hashes_dict.items():\n",
        "    if len(kbo_list) >=2:\n",
        "      kbo_removal_list.extend(kbo_list)\n",
        "  return kbo_removal_list\n",
        "\n",
        "def remove_always(hashes_dict, kbo_removal_list):\n",
        "  delete_all_instances = pd.read_excel('/content/drive/MyDrive/Thesis/duplicates.xlsx')\n",
        "  # Create a list were we will store the bad KBO that acts as example.\n",
        "  # Every hash in the hashed_total_data has a list of associated KBO\n",
        "  # If a kbo found in this list is associated with a specific HASH,\n",
        "  # than all KBO's of that hash need to be deleted\n",
        "  example_bad_kbo = []\n",
        "  for KBO in delete_all_instances['Example KBO']:\n",
        "    example_bad_kbo.append(KBO)\n",
        "  print(example_bad_kbo)\n",
        "  for hash, kbo_list in hashes_dict.items():\n",
        "  # Now find all hashes of the KBO's above and add all associated KBO's to kbo_removal_list\n",
        "    for bad_kbo in example_bad_kbo:\n",
        "      # Check if hash is 'bad'\n",
        "      if bad_kbo in kbo_list:\n",
        "        kbo_removal_list.extend(kbo_list)\n",
        "    return kbo_removal_list\n",
        "\n",
        "def heuristic_removal(hashes_dict, kbo_removal_list):\n",
        "  # our delete heuristic:\n",
        "  # Every hash appearing more than 10 times has been manually checked.\n",
        "  # A number of hashes were completly wrong websites and thus need to be removed\n",
        "  # completly\n",
        "  delete_all_instances = pd.read_excel('/content/drive/MyDrive/Thesis/duplicates.xlsx')\n",
        "  # Create a list were we will store the bad KBO that acts as example.\n",
        "  # Every hash in the hashed_total_data has a list of associated KBO\n",
        "  # If a kbo found in this list is associated with a specific HASH,\n",
        "  # than all KBO's of that hash need to be deleted\n",
        "  example_bad_kbo = []\n",
        "  for KBO in delete_all_instances['Example KBO']:\n",
        "    example_bad_kbo.append(KBO)\n",
        "  print(example_bad_kbo)\n",
        "\n",
        "  # Now we use the dataset containing a sample of some kbo's related to cities and other governments\n",
        "  knu_students = pd.read_csv('/content/drive/MyDrive/Thesis/knu_students.csv')\n",
        "\n",
        "  #Create list of all government KBO's we have available\n",
        "  gov_kbo_list = list()\n",
        "  for index, row in knu_students.iterrows():\n",
        "    gov_kbo_list.append(row['KboNr'])\n",
        "\n",
        "\n",
        "  for hash, kbo_list in hashes_dict.items():\n",
        "    # Now find all hashes of the KBO's above and add all associated KBO's to kbo_removal_list\n",
        "    bad_kbo_found = False\n",
        "    for bad_kbo in example_bad_kbo:\n",
        "      # Check if hash is 'bad'\n",
        "      if bad_kbo in kbo_list:\n",
        "        kbo_removal_list.extend(kbo_list)\n",
        "        bad_kbo_found = True\n",
        "    if not bad_kbo_found:\n",
        "      kbo_gove_boolean = False\n",
        "      # Check for all kbo in kbo_list of current hash if it is a government kbo. If it is than we want to keep only this KBO\n",
        "      for kbo_to_be_handled in kbo_list:\n",
        "        if kbo_to_be_handled in gov_kbo_list:\n",
        "          kbo_list_copy = kbo_list.copy()\n",
        "          kbo_list_copy.remove(kbo_to_be_handled)\n",
        "          kbo_removal_list.extend(kbo_list_copy)\n",
        "          kbo_gove_boolean = True # boolean to check if hash was already handled by this part\n",
        "          break\n",
        "      if not kbo_gove_boolean:\n",
        "        if len(kbo_list) >= 10:\n",
        "          amount_counter = 0\n",
        "          for kbo_to_be_handled in kbo_list:\n",
        "            if amount_counter >= 2:\n",
        "              kbo_removal_list.append(kbo_to_be_handled)\n",
        "            amount_counter += 1\n",
        "\n",
        "        # Check if hash has has more than 1 kbo associated, if so handle it.\n",
        "        elif 10 > len(kbo_list) >= 2:\n",
        "          amount_counter = 0\n",
        "          kbo_gove_boolean = False\n",
        "        # if hash was not resolved by gov check, keep only the KBO number with lowest numerical value (We found that during random checks this was often the correct KBO)\n",
        "          lowest_kbo = 99998765432101\n",
        "          for kbo_to_be_handled in kbo_list:\n",
        "            if int(kbo_to_be_handled) < lowest_kbo:\n",
        "              lowest_kbo = int(kbo_to_be_handled)\n",
        "          for kbo_to_be_handled in kbo_list:\n",
        "            if int(kbo_to_be_handled) == lowest_kbo:\n",
        "              continue\n",
        "            else:\n",
        "              kbo_removal_list.append(kbo_to_be_handled)\n",
        "\n",
        "  kbo_removal_list = list(set(kbo_removal_list))\n",
        "  print(f'Length after removing all hashes selected for total removal {len(kbo_removal_list)}')\n",
        "  return kbo_removal_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LvBzUhn6JdnE"
      },
      "outputs": [],
      "source": [
        "def downcase_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "# Remove numbers from text\n",
        "def remove_numbers(text):\n",
        "  return re.sub(r'\\d+','',text)\n",
        "\n",
        "# Remove special characters and punctuations from text #DONE\n",
        "def remove_punctuations(text):\n",
        "  additional_characters = \"'\\\"©...°”“‘’$€¬±¹£«®\"\n",
        "  all_characters = string.punctuation + additional_characters\n",
        "  return ''.join(char for char in text if char not in all_characters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XMvqHiOQQ7QZ"
      },
      "outputs": [],
      "source": [
        "# Function to remove JavaScript related words from text\n",
        "def remove_javascript(text):\n",
        "  javascript_words = ['javascript', 'js', 'script', 'document', 'function', 'var', 'let', 'const', 'window', 'alert',\n",
        "                    'console', 'return', 'true', 'false', 'if', 'else', 'for', 'while', 'break', 'continue', 'try',\n",
        "                    'catch', 'finally', 'throw', 'new', 'this', 'class', 'instanceof', 'typeof', 'delete', 'in',\n",
        "                    'async', 'await', 'import', 'export', 'module', 'default', 'static', 'extends', 'super', 'break',\n",
        "                    'case', 'switch', 'default', 'yield', 'debugger', 'Infinity', 'NaN', 'isFinite', 'isNaN',\n",
        "                    'parseInt', 'parseFloat', 'undefined']\n",
        "\n",
        "  for word in javascript_words:\n",
        "    word_2 = \" \" + word + \" \"\n",
        "    text = text.replace(word_2, ' ')\n",
        "  return text\n",
        "\n",
        "#Remove cookie/copyright related from text #DONE\n",
        "def remove_cookies_copyright(text):\n",
        "  cookies = ['accepteer', 'accepteren', 'accept', 'advertenties', 'advertisements', 'analyse', 'analyze', 'analytics',\n",
        "            'analytische','analytisch','bepaalde', 'certain', 'bezoeker','visitor','belgian', 'browser','cookies','cookie','copyright',\n",
        "            'choose','kies','kiezen', 'delen', 'share','derden','third', 'parties','party', 'disclaimer', 'functioneel','functional',\n",
        "            'functionele','functioneren','function','gebruik', 'gebruiker','gebruikt', 'use', 'user','used','inhoud',\n",
        "            'content','instellingen','settings', 'klikken','click','login','register','registreer','necessary','noodzakelijk',\n",
        "            'noodzakelijke','opgeslagen', 'opslaan', 'save','saved','pagina','page','policy','beleid','privacy', 'relevante',\n",
        "            'relevant','social','sociaal','sociale', 'store','opslaan','toestemming','consent', 'voorkeuren','preference',\n",
        "            'preferences','website', 'websites','algemene','voorwaarden', 'aanmelden','account','gegevens', 'www', 'com',\n",
        "            'contact', 'contacteer','websites','gebruiken','this', 'these', 'that', 'cookieverklaring', 'toggle', 'more', 'about', 'rights',\n",
        "            'reserved', 'privacy', 'support','ondersteuning', 'copyright', 'cookiebeleid', 'nl', 'fr','eng','de', 'gdpr', 'sitemap']\n",
        "  copyright = ['auteursrecht', 'auteursrechten', 'auteurswet', 'auteursrechtelijk', 'auteursrechtbescherming', 'auteursrechtinbreuk', 'copyrightmelding',\n",
        "            'auteursrechtclaim', 'auteursrechtverklaring', 'auteursrechtvermelding','copyright']\n",
        "  for word in cookies:\n",
        "    word_2 = \" \" + word + \" \"\n",
        "    text = text.replace(word_2,' ')\n",
        "  for word in copyright:\n",
        "    word_2 = \" \" + word + \" \"\n",
        "    text = text.replace(word_2,' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2ZIii0Ah-B-h"
      },
      "outputs": [],
      "source": [
        "#Remove single letters, ' and \" from text #DONE\n",
        "def remove_noise(text):\n",
        "    # Remove single letters\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "    # Remove extra whitespaces and ensure only one whitespace between words\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def remove_null_nace_rows(dataframe):\n",
        "    # Create a mask where True indicates 'NACE' is not null\n",
        "    mask = dataframe['NACE'].notnull()\n",
        "\n",
        "    # Apply the mask to filter the dataframe\n",
        "    filtered_dataframe = dataframe[mask]\n",
        "\n",
        "    return filtered_dataframe\n",
        "\n",
        "def count_words(dataframe, count,min_words):\n",
        "    # Splitting text into words and counting the number of words in each row\n",
        "    dataframe['word_count'] = dataframe['Text'].apply(lambda x: len(str(x).split()))\n",
        "    shape_b = dataframe.shape[0]\n",
        "    dataframe = dataframe[dataframe['word_count'] >= min_words]\n",
        "    shape_a = dataframe.shape[0]\n",
        "    count += (shape_b - shape_a)\n",
        "    # Dropping the 'word_count' column as it's no longer needed\n",
        "    dataframe = dataframe.drop(columns=['word_count'])\n",
        "\n",
        "    return dataframe, count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8tFAjrZcGUhC"
      },
      "outputs": [],
      "source": [
        "def tokenize_ngrams(text):\n",
        "  ngram_range = (3,6)\n",
        "  if len(text) < max(ngram_range):\n",
        "        return ''  # Return empty string if text length is insufficient for n-grams\n",
        "  else:\n",
        "      vectorizer = CountVectorizer(analyzer='char', ngram_range=ngram_range)\n",
        "      try:\n",
        "          X = vectorizer.fit_transform([text])\n",
        "          if X.shape[1] == 0:\n",
        "              return ''  # Return empty string if vocabulary is empty\n",
        "          else:\n",
        "              ngrams = vectorizer.get_feature_names_out()\n",
        "              ngrams = [gram.replace(' ','_') for gram in ngrams]\n",
        "              return ' '.join(ngrams)\n",
        "      except ValueError:\n",
        "          return ''  # Return empty string in case of ValueError\n",
        "\n",
        "def lemmatize_dutch_text(dataframe, nlp, text_column='text'):\n",
        "\n",
        "    # Define a lemmatization function that takes a document and returns its lemmatized version\n",
        "    def lemmatize(doc):\n",
        "        return \" \".join([token.lemma_ for token in nlp(doc)])\n",
        "\n",
        "    # Apply the lemmatization function to the specified text column\n",
        "    dataframe[text_column] = dataframe[text_column].apply(lemmatize)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "def stem_dutch_text(dataframe, stemmer, text_column='Text'):\n",
        "    # Define a stemming function that takes a document and returns its stemmed version\n",
        "    def stem(text):\n",
        "        words = nltk.word_tokenize(text)\n",
        "        return \" \".join([stemmer.stem(word) for word in words])\n",
        "\n",
        "    # Apply the stemming function to the specified text column\n",
        "    dataframe[text_column] = dataframe[text_column].apply(stem)\n",
        "\n",
        "    return dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI6w3ukjUh3N"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # This should be the path to the directory containing the data you want to preform pre processing on\n",
        "  files_directory = '/content/drive/.../'\n",
        "\n",
        "  # Set backup in main to true to first create copy of files before processing it. Copy will be stored in directory below.\n",
        "  backup_files_directory = '/content/drive/.../'\n",
        "\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}