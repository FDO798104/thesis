{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w1vSceE55CR"
      },
      "outputs": [],
      "source": [
        "pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8KYUjIrGFkJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gn7bXWWgh2a"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, f1_score, confusion_matrix\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import fasttext.util\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from openpyxl import load_workbook\n",
        "from scipy.stats import kendalltau\n",
        "import seaborn as sns\n",
        "\n",
        "#fasttext.util.download_model('nl', if_exists='ignore')  # English\n",
        "#ft = fasttext.load_model('cc.nl.300.bin')\n",
        "#ft.get_dimension()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz8FvclOGocD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  ID and information about all pre-processed datasets\n",
        "'''\n",
        "# # 1 == Original pre processing # removed stop words - heuristic duplicate removal - low word count row removal\n",
        "# # 2 == Second pre processing - heuristic duplicate removal - low word count row removal\n",
        "# # 3 == Second pre processing - ALL duplicate removal - low word count row removal\n",
        "# # 4 == Second pre processing - NO duplicate removal - low word count row removal (NOT SURE IF WORD COUNT REMOVAL WAS DONE ON THIS DATA SET)\n",
        "# # 5 == Second pre processing - Lemmatization\n",
        "# # 6 == Second pre processing - Stemming\n",
        "# # 7 == Original pre processing - Stemming\n",
        "# # 8 == Original pre processing - Lemmatization\n",
        "# # 9 == Second pre processing - ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tBRW0YKuCLN"
      },
      "outputs": [],
      "source": [
        "def main(settings_to_run):\n",
        "  '''main function\n",
        "  Args:\n",
        "      settings_to_run (list): List containing all settings to run\n",
        "  Returns:\n",
        "      None\n",
        "  '''\n",
        "  # All settings we want to test for raportation\n",
        "  base_folder = settings_to_run[0]\n",
        "  try:\n",
        "    os.mkdir(base_folder)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  results_folder = settings_to_run[1]\n",
        "  try:\n",
        "    os.mkdir(results_folder)\n",
        "    print(results_folder, 'made')\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  results_file = settings_to_run[2]\n",
        "  if not os.path.exists(results_file):\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df.to_excel(results_file, index=False)\n",
        "\n",
        "  # 6660 for downsampling data based on first number, 100000000 - no downsampling\n",
        "  max_instances_options = settings_to_run[3]\n",
        "  random_state_values = settings_to_run[4]\n",
        "  tf_idf_max_features_options = settings_to_run[5]\n",
        "  iterations_max_options = settings_to_run[6]\n",
        "  use_class_weighting_options = settings_to_run[7]\n",
        "  use_embeddings_options = settings_to_run[8]\n",
        "  pre_selected_use_directory_options = settings_to_run[9]\n",
        "  # create combinations\n",
        "  all_combinations = itertools.product(\n",
        "      max_instances_options, random_state_values, tf_idf_max_features_options,\n",
        "      iterations_max_options, use_class_weighting_options, use_embeddings_options,\n",
        "      pre_selected_use_directory_options\n",
        "  )\n",
        "  filtered_combinations = itertools.product(\n",
        "      max_instances_options, random_state_values, tf_idf_max_features_options,\n",
        "      iterations_max_options, use_class_weighting_options, use_embeddings_options,\n",
        "      pre_selected_use_directory_options\n",
        "  )\n",
        "\n",
        "  # Below some filters are used to not run every combination\n",
        "\n",
        "  # For some datasets we want to test different combos.\n",
        "  # For other datasets we just want to keep the basic settings.\n",
        "  # All non basic settings for those datasets are filtered out here.\n",
        "  remove_embedding = [2,3,4,5,6,7,8,9] # or combo[5] == True\n",
        "  all_combinations = [\n",
        "      combo for combo in all_combinations if not ((combo[5] == True) and combo[6] in remove_embedding)]\n",
        "\n",
        "  # datasets were we only test with downsampling\n",
        "  only_downsampling_true = [9]\n",
        "  filtered_combinations = [\n",
        "      combo for combo in all_combinations if not ((combo[0] != 6660) and combo[6] in only_downsampling_true)]\n",
        "\n",
        "  # First do all with stopword removal as this is the natural reporting combination\n",
        "  true_stop_removal_combinations = [combo for combo in filtered_combinations if combo[-1] == 1]  # Index 5 corresponds to use_embeddings_options\n",
        "  false_stop_removal_combinations = [combo for combo in filtered_combinations if not combo[-1] == 1]\n",
        "\n",
        "  # First do all with stopword removal as this is the natural reporting combination\n",
        "  stopwords_removed = [1,7,8]\n",
        "  true_stop_removal_combinations = [combo for combo in filtered_combinations if combo[-1] in stopwords_removed]  # Index 5 corresponds to use_embeddings_options\n",
        "  false_stop_removal_combinations = [combo for combo in filtered_combinations if not combo[-1] in stopwords_removed]\n",
        "\n",
        "  # filtered_combinations = true_stop_removal_combinations + false_stop_removal_combinations\n",
        "\n",
        "  # Embeddings use more RAM and can not be loaded unless RAM is empty.\n",
        "  # Execute embeddings first\n",
        "  true_embeddings_combinations = [combo for combo in filtered_combinations if combo[5] == True]  # Index 5 corresponds to use_embeddings_options\n",
        "  false_embeddings_combinations = [combo for combo in filtered_combinations if not combo[5] == True]\n",
        "  filtered_combinations = true_embeddings_combinations + false_embeddings_combinations\n",
        "\n",
        "  # NGRAMS use more RAM and are very slow. Can be loaded last.\n",
        "  # Execute ngrams last\n",
        "  true_ngrams_combinations = [combo for combo in filtered_combinations if combo[-1] == 9]\n",
        "  false_ngrams_combinations = [combo for combo in filtered_combinations if not combo[-1] == 9]\n",
        "  filtered_combinations = false_ngrams_combinations + true_ngrams_combinations\n",
        "\n",
        "  # Itertools.product object is consumed once iterated so copies need to be made\n",
        "  # so that we can iterate the combos multiple times for different uses\n",
        "  all_combinations_copies = itertools.tee(filtered_combinations, 5)\n",
        "\n",
        "  for idx, combo in enumerate(all_combinations_copies[0]):\n",
        "        print(f\"Combo {idx + 1}: {combo}\")\n",
        "\n",
        "  iterate_combos(all_combinations_copies[1], base_folder, results_folder, results_file)\n",
        "\n",
        "def iterate_combos(all_combinations, base_folder, results_folder, results_file):\n",
        "  '''iterate all experiment setups\n",
        "  Args:\n",
        "      all_combinations (Itertools.product): itertools product containing all settings for all experiments\n",
        "      base_folder (list): base folder to store all experiments in\n",
        "      results_folder (int): folder to store results files\n",
        "      results_file (str): path to excel file to store results in\n",
        "  Returns:\n",
        "      None\n",
        "  '''\n",
        "  confirmation_input = input(\"Continue? (yes/no)\")\n",
        "  if confirmation_input != 'yes':\n",
        "    raise ValueError('user failed to confirm')\n",
        "  counter = 1\n",
        "  for idx, combination in enumerate(all_combinations):\n",
        "    if counter <= 0:\n",
        "      counter += 1\n",
        "      print(f\"skipping {idx}\")\n",
        "      continue\n",
        "    else:\n",
        "      counter +=1\n",
        "      first_models_directory, second_models_directory, third_models_directory, \\\n",
        "      main_experiment_folder, settings_string = create_model_folders(base_folder, combination, idx)\n",
        "      print(f'running experiment according to settings: {settings_string}')\n",
        "\n",
        "      # Assign variables from combination\n",
        "      max_instances, random_state_value, tf_idf_max_features, iterations_max, \\\n",
        "      use_class_weighting, use_embeddings, use_directory = combination\n",
        "\n",
        "      # Depending on use_directory, set the appropriate data_directory\n",
        "      data_directory = select_directory(use_directory)\n",
        "      print(f'The selcted data directory is: {data_directory}')\n",
        "      # run the experiment\n",
        "      run_experiment(max_instances, random_state_value, tf_idf_max_features,\\\n",
        "                            iterations_max, use_class_weighting, use_embeddings,\\\n",
        "                            data_directory, base_folder, results_folder, results_file,\\\n",
        "                            first_models_directory, second_models_directory, third_models_directory, \\\n",
        "                            main_experiment_folder, settings_string, combination, idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC5LXxGxhKgb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(max_instances,\n",
        "                   random_state_value,\n",
        "                   tf_idf_max_features,\n",
        "                   iterations_max,\n",
        "                   use_class_weighting,\n",
        "                   use_embeddings,\n",
        "                   data_directory,\n",
        "                   base_folder,\n",
        "                   results_folder,\n",
        "                   results_file,\n",
        "                   first_models_directory,\n",
        "                   second_models_directory,\n",
        "                   third_models_directory,\n",
        "                   main_experiment_folder,\n",
        "                   settings_string,\n",
        "                   combination, idx):\n",
        "  ''' Prepares data for Logistic Regression models\n",
        "\n",
        "  Args:\n",
        "      max_instances (int): max instances per NACEBEL first digit.\n",
        "      random_state_value (int): random state for reproducability\n",
        "      tf_idf_max_features (int): the max number of features for TF-IDF\n",
        "      iterations_max (int): max number of allowed iterations for convergence\n",
        "      use_class_weighting (Boolean): True if class weighting should be used\n",
        "      use_embeddings (Boolean): use embeddings instead of TF-IDF\n",
        "      data_directory (string): directory containing data\n",
        "      base_folder (string): base folder for all experiments\n",
        "      results_folder (string): results folder for all experiments\n",
        "      first_models_directory (string): first model directory for this experiment\n",
        "      second_models_directory (string): second digit models directory for this experiment\n",
        "      third_models_directory (string): final digits models directory for this experiment\n",
        "      main_experiment_folder (string): main folder for this specific experiment\n",
        "      settings_string (string): string with all settings used for naming results tab\n",
        "      combination (list): list containing settings for experiment\n",
        "      idx (int): experiment id\n",
        "  Returns:\n",
        "      None\n",
        "  '''\n",
        "\n",
        "  print(data_directory)\n",
        "  df = load_data(data_directory)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = feature_handling(max_instances,\n",
        "                                                      random_state_value,\n",
        "                                                      tf_idf_max_features,\n",
        "                                                      use_embeddings,\n",
        "                                                      df)\n",
        "\n",
        "  first_digit_model_path = train_first_digit_model(X_train, y_train, use_class_weighting, first_models_directory, iterations_max)\n",
        "\n",
        "  #second_digit_models =\n",
        "  train_second_digit_model(X_train, y_train, use_class_weighting, second_models_directory, iterations_max)\n",
        "\n",
        "  #remaining_digit_models =\n",
        "  train_remaining_digit_model(X_train, y_train, use_class_weighting, third_models_directory, iterations_max)\n",
        "\n",
        "  test_model_perormance(X_test, y_test,\n",
        "                          first_digit_model_path,\n",
        "                          second_models_directory,\n",
        "                          third_models_directory,\n",
        "                          results_folder,\n",
        "                          results_file,\n",
        "                          settings_string, idx, combination)\n",
        "\n",
        "\n",
        "def create_model_folders(base_folder, combination, idx):\n",
        "  '''Creates file structure\n",
        "Args:\n",
        "    base_folder (str): max instances per NACEBEL first digit.\n",
        "    combination (list): list containing settings for experiment\n",
        "    idx (int): id of experiment\n",
        "Returns:\n",
        "    string: directory for first model\n",
        "    string: directory for all second digit models\n",
        "    string: directory for all third digit models\n",
        "    string: main folder for this specific experiment\n",
        "    string: string with all settings used for naming results tab\n",
        "'''\n",
        "  if combination[5]:\n",
        "    temp_string = f'EMB'\n",
        "  else:\n",
        "    temp_string = f'TFIDF[{combination[2]}]'\n",
        "\n",
        "  settings_string = f'{idx+1}:inst:{combination[0]}_{temp_string}_CWEIGHT:{combination[4]}_DATA{combination[6]}'\n",
        "  main_experiment_folder = os.path.join(base_folder,settings_string)\n",
        "  try:\n",
        "    os.mkdir(main_experiment_folder)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  first_models_directory = os.path.join(main_experiment_folder,'1_first_digit')\n",
        "  try:\n",
        "    os.mkdir(first_models_directory)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  second_models_directory = os.path.join(main_experiment_folder,'2_second_digit')\n",
        "  try:\n",
        "    os.mkdir(second_models_directory)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  third_models_directory = os.path.join(main_experiment_folder,'3_remaining_digit')\n",
        "  try:\n",
        "    os.mkdir(third_models_directory)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "  return first_models_directory, second_models_directory, third_models_directory, main_experiment_folder, settings_string\n",
        "\n",
        "# Based on use directory setting the path to the pre processed files for this setting will be used\n",
        "def select_directory(use_directory):\n",
        "  ''' Load data from parquet files\n",
        "  Args:\n",
        "      use_directory (int): integer representing chosen pre processed saved data files\n",
        "  Returns:\n",
        "      str: directory containing data files\n",
        "\n",
        "  '''\n",
        "  print(f'use_directory is {use_directory}')\n",
        "  if use_directory == 1:\n",
        "    data_directory = '/content/drive/MyDrive/Thesis/textdata_files/reduced_preprocessed_data/AllData'\n",
        "  elif use_directory == 2:\n",
        "    data_directory = '/content/drive/MyDrive/Thesis/textdata_files/pre_processed_beta/AllData'\n",
        "  elif use_directory == 3:\n",
        "    data_directory = '/content/drive/MyDrive/Thesis/textdata_files/pre_processed_beta/AllData_unique'\n",
        "  elif use_directory == 4:\n",
        "    data_directory = '/content/drive/MyDrive/Thesis/textdata_files/pre_processed_beta/AllData_all_rows/processed'\n",
        "  elif use_directory == 5:\n",
        "    data_directory = '/content/drive/MyDrive/Thesis/textdata_files/pre_processed_beta/Lemmatization'\n",
        "  elif use_directory == 6:\n",
        "    data_directory = '/content/drive/MyDrive/Thesis/textdata_files/pre_processed_beta/Stemming'\n",
        "  elif use_directory == 7:\n",
        "    data_directory = '/content/drive/MyDrive/Thesis/textdata_files/pre_processed_beta/Stemming_2'\n",
        "  elif use_directory == 8:\n",
        "    data_directory = '/content/drive/MyDrive/Thesis/textdata_files/pre_processed_beta/Lemmatization_2'\n",
        "  elif use_directory == 9:\n",
        "    data_directory = '/content/drive/MyDrive/Thesis/textdata_files/pre_processed_beta/NGrams'\n",
        "  else:\n",
        "    raise ValueError('use_directory should be a value between 1 and 9')\n",
        "  return data_directory\n",
        "\n",
        "# Load data\n",
        "def load_data(data_directory):\n",
        "  ''' Load data from parquet files\n",
        "  Args:\n",
        "      data_directory (str): directory containing data files\n",
        "  Returns:\n",
        "      Pandas DataFrame: dataframe containing all the data read from the files\n",
        "\n",
        "  '''\n",
        "  print(f'loading data from {data_directory}...')\n",
        "  # Get a list of all files in the directory\n",
        "  data_files = os.listdir(data_directory)\n",
        "\n",
        "  # Initialize an empty list to store DataFrames\n",
        "  dataframes = []\n",
        "\n",
        "  # Iterate through each file in the directory\n",
        "  for file in data_files:\n",
        "      if file.endswith('.parquet'):\n",
        "          # Read the file and append its DataFrame to the list\n",
        "          file_path = os.path.join(data_directory, file)\n",
        "          dataframes.append(pd.read_parquet(file_path))\n",
        "  # Format NACE code so that if len == 4 the missing zero (excel for data storage = trouble) is added to the front\n",
        "  def format_nace(code):\n",
        "    return '0' + code if len(code) == 4 else code\n",
        "\n",
        "  df = pd.concat(dataframes)\n",
        "  # Apply the function to the 'NACE' column\n",
        "  df['NACE'] = df['NACE'].apply(format_nace)\n",
        "\n",
        "  # Concatenate all DataFrames into a single DataFrame\n",
        "  df = df[['NACE','Text']]\n",
        "\n",
        "  df\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsVUnXjPiyW7"
      },
      "outputs": [],
      "source": [
        "def feature_handling(max_instances, random_state_value, tf_idf_max_features, use_embeddings, df):\n",
        "  ''' Prepares data for Logistic Regression models\n",
        "\n",
        "  Args:\n",
        "      max_instances (int): max instances per NACEBEL first digit.\n",
        "      random_state_value (int): random state for reproducability\n",
        "      tf_idf_max_features (int): the max number of features for TF-IDF\n",
        "      use_embeddings (Boolean): use embeddings instead of TF-IDF\n",
        "      df (pandas DataFrame): dataframe with columns ('NACE', 'Text')\n",
        "  Returns:\n",
        "      NumPy array: training dataset texts\n",
        "      NumPy array: test dataset texts\n",
        "      NumPy array: training dataset NACEBEL CODES\n",
        "      NumPy array: test dataset NACEBEL codes\n",
        "\n",
        "  '''\n",
        "  print(f'feature handling in progress. Downsampling: {max_instances}, embeddings: {use_embeddings}...')\n",
        "  # Important, the path to fasttext file should be stored here (large file)\n",
        "  fasttext_nl_path = '/content/drive/MyDrive/Thesis/models/EMBEDDINGS/cc.nl.300.bin'\n",
        "\n",
        "  max_instances_per_first_digit = max_instances\n",
        "\n",
        "  # Extract first digit of NACE codes and create a new column\n",
        "  df['First_NACE'] = df['NACE'].astype(str).str[0]\n",
        "\n",
        "  # Print the total size of the dataset before sampling\n",
        "  print(\"Total size of dataset before sampling:\", len(df))\n",
        "\n",
        "  print(\"Number of instances before sampling:\")\n",
        "  print(df['First_NACE'].value_counts())\n",
        "  df = df[df['First_NACE'] != 'X']\n",
        "  # Plot the distribution of first digits before sampling\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  df.groupby('First_NACE').size().plot(kind='bar',  title='Before Sampling')\n",
        "  print(df.groupby('First_NACE').size().median())\n",
        "\n",
        "  # Initialize a list to store resampled DataFrames\n",
        "  resampled_dfs = []\n",
        "\n",
        "  # Iterate over each unique first digit\n",
        "  for first_digit in range(0, 10):\n",
        "      # Filter DataFrame for NACE codes starting with the current first digit\n",
        "      df_x = df[df[\"NACE\"].str.startswith(str(first_digit))]\n",
        "\n",
        "      # If the DataFrame is empty, continue to the next iteration\n",
        "      if df_x.empty:\n",
        "          continue\n",
        "\n",
        "      # Check if the number of instances for this first digit is greater than CHOOSE\n",
        "      if len(df_x) > max_instances_per_first_digit:\n",
        "          # Downsample the DataFrame to CHOOSE instances using resample - REPLACE SHOULD BE FALSE\n",
        "          df_x_downsampled = resample(df_x, replace=False, n_samples=max_instances_per_first_digit, random_state=random_state_value)\n",
        "          resampled_dfs.append(df_x_downsampled)\n",
        "      else:\n",
        "          resampled_dfs.append(df_x)\n",
        "\n",
        "  # Concatenate the resampled DataFrames\n",
        "  resampled_df = pd.concat(resampled_dfs)\n",
        "\n",
        "  # Detele df's so that RAM gets cleared.\n",
        "  #del df\n",
        "  for df_x in resampled_dfs:\n",
        "    del df_x\n",
        "  del resampled_dfs\n",
        "\n",
        "  # Split resampled data into features and target\n",
        "  X = resampled_df['Text']\n",
        "  y = resampled_df['NACE']\n",
        "\n",
        "  # # Vectorize the text data using TF-IDF\n",
        "  if use_embeddings == False:\n",
        "    print(f'vectorizing using TF_IDF')\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=tf_idf_max_features, sublinear_tf=True) # Uses euclidean norm (l2)\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=random_state_value)\n",
        "\n",
        "  else:\n",
        "    # load embedding model (fasttext)\n",
        "    print(f'Embedding using fasttext (DUTCH)')\n",
        "    ft = fasttext.load_model(fasttext_nl_path)\n",
        "    embeddings = []\n",
        "    # Iterate over each text sample in X\n",
        "    for text in X:\n",
        "        # Get the embedding for the text using the loaded FastText model\n",
        "        embedding = ft.get_sentence_vector(text)\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    # Convert the list of embeddings to a numpy array\n",
        "    X_embeddings = np.array(embeddings)\n",
        "\n",
        "    # Split the embedded data and the target into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=random_state_value)\n",
        "\n",
        "  # Print the total size of the dataset after sampling\n",
        "  print(\"\\nTotal size of dataset after sampling:\", len(resampled_df))\n",
        "  print(\"\\nSize of train set:\", X_train.shape[0])\n",
        "  print(\"\\nSize of test set:\", X_test.shape[0])\n",
        "\n",
        "  # Print the number of instances after sampling\n",
        "  print(\"\\nNumber of instances after sampling:\")\n",
        "  print(resampled_df[\"NACE\"].str[0].value_counts())\n",
        "\n",
        "  # Plot the distribution of first digits after sampling\n",
        "  plt.subplot(1, 2, 2)\n",
        "  resampled_df.groupby('First_NACE').size().plot(kind='bar', title='After Sampling')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  num_labels = len(resampled_df['NACE'].unique())\n",
        "  unique_nace_resampled = resampled_df['NACE'].unique()\n",
        "\n",
        "  # Get unique NACE codes in df\n",
        "  unique_nace_df = df['NACE'].unique()\n",
        "\n",
        "  # Find NACE codes in df that are not present in resampled_df\n",
        "  missing_nace = set(unique_nace_df) - set(unique_nace_resampled)\n",
        "\n",
        "  # Count rows in df with missing NACE codes\n",
        "  count_missing_rows = df[df['NACE'].isin(missing_nace)].shape[0]\n",
        "  print(f'number of labels resampled: {num_labels} - number of labels original {len(unique_nace_df)} - number of instances deleted from these categories: {count_missing_rows}')\n",
        "\n",
        "  return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol26ZQia17sT"
      },
      "source": [
        "# Training models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcTC1APfi81L"
      },
      "outputs": [],
      "source": [
        "def train_first_digit_model(X_train, y_train, use_class_weighting, first_models_directory, iterations_max):\n",
        "  ''' code to train first digit model\n",
        "\n",
        "  Args:\n",
        "      X_train (NumPy array): training dataset texts\n",
        "      y_train (NumPy array): training dataset NACEBEL codes\n",
        "      use_class_weighting (Boolean): Apply class weighting or not\n",
        "      first_models_directory (str): directory for first model to be stored\n",
        "      iterations_max (int): max iterations limit\n",
        "  Returns:\n",
        "      string: first figit model path\n",
        "\n",
        "  '''\n",
        "  # Train a model to predict the first digit\n",
        "  y_train_first_digit = y_train.str[0]  # Extracting only the first digit from the labels\n",
        "  print(\"Training first digit model...\")\n",
        "  print(f\"Number of samples for training: {X_train.shape[0]}\")\n",
        "  print(y_train_first_digit)\n",
        "\n",
        "  if use_class_weighting == True:\n",
        "    class_labels = sorted(set(y_train_first_digit))  # Get unique class labels\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=class_labels, y=y_train_first_digit)\n",
        "\n",
        "    # Create a dictionary mapping class labels to class weights\n",
        "    class_weight_dict = dict(zip(class_labels, class_weights))\n",
        "    first_digit_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=iterations_max,  class_weight=class_weight_dict)\n",
        "\n",
        "  else:\n",
        "    # All models use L2 regularization - not specifically defined everywhere as it is standard for this funciton\n",
        "    first_digit_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=iterations_max)\n",
        "\n",
        "  first_digit_model.fit(X_train, y_train_first_digit)\n",
        "\n",
        "  # Save the first digit model\n",
        "  first_digit_model_path = os.path.join(first_models_directory, 'first_digit_model.pkl')\n",
        "  joblib.dump(first_digit_model, first_digit_model_path)\n",
        "  print(\"First digit model trained and saved.\")\n",
        "  return first_digit_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_Hz7PlQ5xcA"
      },
      "outputs": [],
      "source": [
        "def train_second_digit_model(X_train, y_train, use_class_weighting, second_models_directory, iterations_max):\n",
        "  ''' code to train second digit models\n",
        "\n",
        "  Args:\n",
        "      X_train (NumPy array): training dataset texts\n",
        "      y_train (NumPy array): training dataset NACEBEL codes\n",
        "      use_class_weighting (Boolean): Apply class weighting or not\n",
        "      second_models_directory (str): directory for second digit models to be stored\n",
        "      iterations_max (int): max iterations limit\n",
        "  Returns:\n",
        "    None\n",
        "  '''\n",
        "  second_digit_models = {}\n",
        "  # Train a model for every first NACEBEL digit\n",
        "  for first_digit in ['0','1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
        "      X_train_second = X_train[y_train.str[0] == first_digit]\n",
        "      y_train_second = y_train[y_train.str[0] == first_digit].str[1]\n",
        "      # Check if there are samples available for training\n",
        "      if X_train_second.shape[0] == 0:\n",
        "          print(f\"No samples available for training for first digit {first_digit}. Skipping.\")\n",
        "          continue\n",
        "\n",
        "      print(f\"Training second digit model for first digit {first_digit}...\")\n",
        "      print(f\"Number of samples for training: {X_train_second.shape[0]}\")\n",
        "      if use_class_weighting == True:\n",
        "        class_labels = sorted(set(y_train_second))  # Get unique class labels\n",
        "\n",
        "        # Compute class weights\n",
        "        class_weights = compute_class_weight('balanced', classes=class_labels, y=y_train_second)\n",
        "\n",
        "        # Create a dictionary mapping class labels to class weights\n",
        "        class_weight_dict = dict(zip(class_labels, class_weights))\n",
        "        second_digit_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=iterations_max,  class_weight=class_weight_dict)\n",
        "\n",
        "      else:\n",
        "        second_digit_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=iterations_max)\n",
        "\n",
        "      second_digit_model.fit(X_train_second, y_train_second)\n",
        "\n",
        "      # Save the second digit model\n",
        "      second_digit_model_filename = f'second_digit_model_{first_digit}.pkl'\n",
        "      second_digit_model_path = os.path.join(second_models_directory, second_digit_model_filename)\n",
        "      joblib.dump(second_digit_model, second_digit_model_path)\n",
        "      second_digit_models[first_digit] = second_digit_model\n",
        "\n",
        "      print(f\"Second digit model for first digit {first_digit} trained and saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agFyRqZV5z0G"
      },
      "outputs": [],
      "source": [
        "def train_remaining_digit_model(X_train, y_train, use_class_weighting, third_models_directory, iterations_max):\n",
        "  ''' code to train final digits models\n",
        "\n",
        "  Args:\n",
        "      X_train (NumPy array): training dataset texts\n",
        "      y_train (NumPy array): training dataset NACEBEL codes\n",
        "      use_class_weighting (Boolean): Apply class weighting or not\n",
        "      third_models_directory (str): directory for final digits models to be stored\n",
        "      iterations_max (int): max iterations limit\n",
        "  Returns:\n",
        "    None\n",
        "  '''\n",
        "  # Train models to predict the remaining three digits for each combination of first and second digits\n",
        "  remaining_digit_models = {}\n",
        "\n",
        "  # Delete all stored models to make sure only new correct models are in folder\n",
        "  for file in os.listdir(third_models_directory):\n",
        "      # Check if the file ends with '.pkl'\n",
        "      if file.endswith(\".pkl\"):\n",
        "          # Construct the file path\n",
        "          file_path = os.path.join(third_models_directory, file)\n",
        "          # Delete the file\n",
        "          os.remove(file_path)\n",
        "  # There is model for every digit\n",
        "  for first_digit in range(0, 10):\n",
        "      print(f\"Training models for first digit {first_digit}...\")\n",
        "      for second_digit in ['0','1', '2', '3', '4', '5', '6', '7', '8', '9', 'X']:\n",
        "          print(f\"    Training models for second digit {second_digit}...\")\n",
        "          X_train_third = X_train[(y_train.str[0] == str(first_digit)) & (y_train.str[1] == str(second_digit))]\n",
        "          y_train_third = y_train[(y_train.str[0] == str(first_digit)) & (y_train.str[1] == str(second_digit))].str[2:]\n",
        "\n",
        "          # Check if there are samples available for training\n",
        "          if X_train_third.shape[0] == 0:\n",
        "              print(f\"        No samples available for training for first digit {first_digit}, second digit {second_digit}. Skipping.\")\n",
        "              continue\n",
        "\n",
        "          # Check if there are at least two unique classes in the target variable\n",
        "          unique_classes = Counter(y_train_third)\n",
        "          if len(unique_classes) < 2:\n",
        "              print(f\"        Only one class present in the data for first digit {first_digit}, second digit {second_digit}. Skipping.\")\n",
        "              continue\n",
        "\n",
        "          if use_class_weighting == True:\n",
        "            class_labels = sorted(set(y_train_third))  # Get unique class labels\n",
        "\n",
        "            # Compute class weights\n",
        "            class_weights = compute_class_weight('balanced', classes=class_labels, y=y_train_third)\n",
        "\n",
        "            # Create a dictionary mapping class labels to class weights\n",
        "            class_weight_dict = dict(zip(class_labels, class_weights))\n",
        "            third_digit_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=iterations_max,  class_weight=class_weight_dict)\n",
        "          else:\n",
        "            third_digit_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=iterations_max)\n",
        "\n",
        "          third_digit_model.fit(X_train_third, y_train_third)\n",
        "\n",
        "          # Save the third digit model\n",
        "          third_digit_model_filename = f'remaining_digit_model_{first_digit}_{second_digit}.pkl'\n",
        "          third_digit_model_path = os.path.join(third_models_directory, third_digit_model_filename)\n",
        "          joblib.dump(third_digit_model, third_digit_model_path)\n",
        "          remaining_digit_models[(first_digit, second_digit)] = third_digit_model\n",
        "          print(f\"        Third digit model trained for first digit {first_digit}, second digit {second_digit}.\")\n",
        "  #return remaining_digit_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CODyaGh6-oKb"
      },
      "outputs": [],
      "source": [
        "def test_model_perormance(X_test, y_test,\n",
        "                          first_digit_model_path,\n",
        "                          second_models_directory,\n",
        "                          third_models_directory,\n",
        "                          results_folder,\n",
        "                          results_file,\n",
        "                          settings_string, idx, combination):\n",
        "  ''' test models performance\n",
        "  Args:\n",
        "      X_test (NumPy array): test dataset texts\n",
        "      y_test (NumPy array): test dataset NACEBEL codes\n",
        "      use_class_weighting (Boolean): Apply class weighting or not\n",
        "      first_digit_model_path (str): path were first digit model is stored\n",
        "      second_models_directory (str): path were second digit models are stored\n",
        "      third_models_directory (str): path were final digits models are stored\n",
        "      results_folder (str): path to results folder to save confusion matrix\n",
        "      results_file (str): path to excel file were all experiment results have to be saved\n",
        "      settings_string (str): string containing settings of experiment to name it\n",
        "      idx (str): ID\n",
        "      combination (int): settings of this experiment\n",
        "  Returns:\n",
        "    None\n",
        "  '''\n",
        "  # Load first digit model\n",
        "  first_digit_model = joblib.load(first_digit_model_path)\n",
        "\n",
        "  # Load second digit models\n",
        "  second_digit_models = {}\n",
        "  for first_digit in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
        "      second_digit_model_path = os.path.join(second_models_directory, f'second_digit_model_{first_digit}.pkl')\n",
        "      second_digit_models[first_digit] = joblib.load(second_digit_model_path)\n",
        "\n",
        "  # Load third digit models\n",
        "  third_digit_models = {}\n",
        "  for first_digit in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
        "    for second_digit in ['0','1', '2', '3', '4', '5', '6', '7', '8', '9', 'X']:\n",
        "      third_digit_model_path = os.path.join(third_models_directory, f'remaining_digit_model_{first_digit}_{second_digit}.pkl')\n",
        "      if os.path.exists(third_digit_model_path):\n",
        "        third_digit_models[(first_digit, second_digit)] = joblib.load(third_digit_model_path)\n",
        "\n",
        "\n",
        "  # Perform inference for each test instance\n",
        "  y_pred_nace = []\n",
        "  for i in range(X_test.shape[0]):\n",
        "      # Predict first digit\n",
        "      #X_test = X_test[i].reshape(-1,1)\n",
        "      #first_digit_pred = first_digit_model.predict(X_test[i])[0]\n",
        "      first_digit_pred = first_digit_model.predict(X_test[i].reshape(-1,X_test.shape[1]))[0]\n",
        "\n",
        "      # Select the corresponding second digit model\n",
        "      second_digit_model = second_digit_models[first_digit_pred]\n",
        "\n",
        "      # Predict second digit\n",
        "      # second_digit_pred = second_digit_model.predict(X_test[i])[0]\n",
        "      second_digit_pred = second_digit_model.predict(X_test[i].reshape(-1,X_test.shape[1]))[0]\n",
        "      # Check if there is a model for the third digit\n",
        "      if (first_digit_pred, second_digit_pred) in third_digit_models:\n",
        "          third_digit_model = third_digit_models[(first_digit_pred, second_digit_pred)]\n",
        "\n",
        "          # Predict third digit\n",
        "          #third_digit_pred = third_digit_model.predict(X_test[i])[0]\n",
        "          third_digit_pred = third_digit_model.predict(X_test[i].reshape(-1,X_test.shape[1]))[0]\n",
        "      else:\n",
        "          # If no model is present, predict 'X' as a placeholder for the third digit\n",
        "          third_digit_pred = 'XXX'\n",
        "\n",
        "      # Combine predictions to form NACE code\n",
        "      nace_code_pred = f\"{first_digit_pred}{second_digit_pred}{third_digit_pred}\"\n",
        "      y_pred_nace.append(nace_code_pred)\n",
        "\n",
        "  # Metrics\n",
        "  unique_classes = set([string[:1] for string in y_test] + [string[:1] for string in y_pred_nace])\n",
        "\n",
        "  cm = confusion_matrix([string[:1] for string in y_test], [string[:1] for string in y_pred_nace])\n",
        "\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=sorted(unique_classes), yticklabels = sorted(unique_classes))\n",
        "  plt.xlabel('Predicted Labels')\n",
        "  plt.ylabel('True Labels')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.savefig(os.path.join(results_folder, f'{settings_string}_confusion_matrix.png'))\n",
        "  plt.show()\n",
        "\n",
        "  accuracy5 = accuracy_score(y_test, y_pred_nace)\n",
        "  accuracy4 = accuracy_score([string[:4] for string in y_test], [string[:4] for string in y_pred_nace])\n",
        "  accuracy3 = accuracy_score([string[:3] for string in y_test], [string[:3] for string in y_pred_nace])\n",
        "  accuracy2 = accuracy_score([string[:2] for string in y_test], [string[:2] for string in y_pred_nace])\n",
        "  accuracy1 = accuracy_score([string[:1] for string in y_test], [string[:1] for string in y_pred_nace])\n",
        "\n",
        "  accuracy_of_345 = accuracy_score([string[2:5] for string in y_test], [string[2:5] for string in y_pred_nace])\n",
        "  accuracy_of_2 = accuracy_score([string[1] for string in y_test], [string[1] for string in y_pred_nace])\n",
        "  accuracy_of_1 = accuracy_score([string[0] for string in y_test], [string[0] for string in y_pred_nace])\n",
        "  print(\"Accuracy (1 digit):\", accuracy1)\n",
        "  print(\"Accuracy (2 digit):\", accuracy2)\n",
        "  print(\"Accuracy (3 digit):\", accuracy3)\n",
        "  print(\"Accuracy (4 digit):\", accuracy4)\n",
        "  print(\"Overall Accuracy:\", accuracy5)\n",
        "\n",
        "  print(\"Accuracy of 1 digit model:\", accuracy_of_1)\n",
        "  print(\"Accuracy of 2nd digit model:\", accuracy_of_2)\n",
        "  print(\"Accuracy of 3,4 and 5th digit model):\", accuracy_of_345)\n",
        "\n",
        "  f1_score5 = f1_score(y_test, y_pred_nace, average = 'weighted')\n",
        "  f1_score4 = f1_score([string[:4] for string in y_test], [string[:4] for string in y_pred_nace], average = 'weighted')\n",
        "  f1_score3 = f1_score([string[:3] for string in y_test], [string[:3] for string in y_pred_nace], average = 'weighted')\n",
        "  f1_score2 = f1_score([string[:2] for string in y_test], [string[:2] for string in y_pred_nace], average = 'weighted')\n",
        "  f1_score1 = f1_score([string[:1] for string in y_test], [string[:1] for string in y_pred_nace], average = 'weighted')\n",
        "\n",
        "  f1_score_of345 = f1_score([string[2:5] for string in y_test], [string[2:5] for string in y_pred_nace], average = 'weighted')\n",
        "  f1_score_of2 = f1_score([string[1] for string in y_test], [string[1] for string in y_pred_nace], average = 'weighted')\n",
        "  f1_score_of1 = f1_score([string[0] for string in y_test], [string[0] for string in y_pred_nace], average = 'weighted')\n",
        "  print(\"F1 (1 digit):\", f1_score1)\n",
        "  print(\"F1 (2 digit):\", f1_score2)\n",
        "  print(\"F1 (3 digit):\", f1_score3)\n",
        "  print(\"F1 (4 digit):\", f1_score4)\n",
        "  print(\"Overall F1:\", f1_score5)\n",
        "\n",
        "  print(\"F1 of 1 digit model:\", f1_score_of1)\n",
        "  print(\"F1 of 2nd digit model:\", f1_score_of2)\n",
        "  print(\"F1 of 3,4 and 5th digit model):\", f1_score_of345)\n",
        "\n",
        "  # kendall's tau metric is not used in final version of thesis.\n",
        "  kendalltau5, p5 = kendalltau(y_test, y_pred_nace)\n",
        "  kendalltau4, p5 = kendalltau([string[:4] for string in y_test], [string[:4] for string in y_pred_nace])\n",
        "  kendalltau3, p5 = kendalltau([string[:3] for string in y_test], [string[:3] for string in y_pred_nace])\n",
        "  kendalltau2, p5 = kendalltau([string[:2] for string in y_test], [string[:2] for string in y_pred_nace])\n",
        "  kendalltau1, p5 = kendalltau([string[:1] for string in y_test], [string[:1] for string in y_pred_nace])\n",
        "\n",
        "  kendalltau_of345, p5_ = kendalltau([string[2:5] for string in y_test], [string[2:5] for string in y_pred_nace])\n",
        "  kendalltau_of2, p5_ = kendalltau([string[1] for string in y_test], [string[1] for string in y_pred_nace])\n",
        "  kendalltau_of1, p5_ = kendalltau([string[0] for string in y_test], [string[0] for string in y_pred_nace])\n",
        "  print(\"Kendall tau (1 digit):\", kendalltau1)\n",
        "  print(\"Kendall tau (2 digit):\", kendalltau2)\n",
        "  print(\"Kendall tau (3 digit):\", kendalltau3)\n",
        "  print(\"Kendall tau (4 digit):\", kendalltau4)\n",
        "  print(\"Overall Kendall tau:\", kendalltau5)\n",
        "  print(\"Kendall tau of 1 digit model:\", kendalltau_of1)\n",
        "  print(\"Kendall tau of 2nd digit model:\", kendalltau_of2)\n",
        "  print(\"Kendall tau of 3,4 and 5th digit model):\", kendalltau_of345)\n",
        "\n",
        "  # Calculate accuracies, F1 scores, and Kendall tau values\n",
        "  accuracies = [accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, 0]\n",
        "  accuracies_of = [accuracy_of_1, accuracy_of_2, 0, 0,0,accuracy_of_345]\n",
        "  f1_scores = [f1_score1, f1_score2, f1_score3, f1_score4, f1_score5, 0]\n",
        "  f1_scores_of = [f1_score_of1, f1_score_of2, 0, 0, 0, f1_score_of345]\n",
        "  kendall_taus = [kendalltau1, kendalltau2, kendalltau3, kendalltau4, kendalltau5, 0]\n",
        "  kendall_taus_of = [kendalltau_of1, kendalltau_of2, 0, 0, 0, kendalltau_of345]\n",
        "\n",
        "  data_set_string = 'None'\n",
        "  stop_removed = True\n",
        "  downsampled = True\n",
        "\n",
        "  pre_processing_string = 'None'\n",
        "  if combination[0] > 90000:\n",
        "    downsampled = False\n",
        "  features_string = f'TF-IDF-{combination[2]}'\n",
        "  feature_string_2 = 'TFIDF'\n",
        "  if combination[5]:\n",
        "    features_string = 'Embeddings'\n",
        "    feature_string_2 = 'EMBED'\n",
        "\n",
        "  if combination[-1] == 1:\n",
        "    data_set_string = 'NO_STOPWORDS_NONE'\n",
        "  elif combination[-1] == 2:\n",
        "    data_set_string = 'YES_STOPWORDS_NONE'\n",
        "    stop_removed = False\n",
        "  elif combination[-1] == 3:\n",
        "    data_set_string = 'YES_STOPWORDS_NONE_ALL_DELETED'\n",
        "    stop_removed = False\n",
        "  elif combination[-1] == 4:\n",
        "    data_set_string = 'YES_STOPWORDS_NONE_NONE_DELETED'\n",
        "    stop_removed = False\n",
        "  elif combination[-1] == 5:\n",
        "    data_set_string = 'YES_STOPWORDS_LEM'\n",
        "    pre_processing_string = 'LEM'\n",
        "    stop_removed = False\n",
        "  elif combination[-1] == 6:\n",
        "    data_set_string = 'NO_STOPWORDS_STEM'\n",
        "    pre_processing_string = 'STEM'\n",
        "    stop_removed = False\n",
        "  elif combination[-1] == 7:\n",
        "    data_set_string = 'NO_STOPWORDS_STEM'\n",
        "    pre_processing_string = 'STEM'\n",
        "  elif combination[-1] == 8:\n",
        "    data_set_string = 'NO_STOPWORDS_LEM'\n",
        "    pre_processing_string = 'LEM'\n",
        "  elif combination[-1] == 9:\n",
        "    data_set_string = 'YES_STOPWORDS_NGRAM'\n",
        "    pre_processing_string = 'NGRAM'\n",
        "    stop_removed = False\n",
        "\n",
        "  sheet_name = (f'STOP={str(stop_removed)}_PRE={pre_processing_string}_FT={feature_string_2}_DS={str(downsampled)}_CW={str(combination[4])}')\n",
        "\n",
        "  settings_df = pd.DataFrame({\n",
        "   'Downsampled':[str(downsampled)],\n",
        "   'features':[features_string],\n",
        "   'class_weighting':[str(combination[4])],\n",
        "    'data_dir':[combination[6]],\n",
        "   'data_string':[data_set_string]})\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      'Accuracy': accuracies,\n",
        "      'F1 Score': f1_scores,\n",
        "      'Kendall Tau': kendall_taus,\n",
        "      'accuracy_of': accuracies_of,\n",
        "      'f1_scores_of': f1_scores_of,\n",
        "      'kendall Tau_of': kendall_taus_of,\n",
        "      'Digit Length': [1, 2, 3, 4, 5, 345]\n",
        "  })\n",
        "  # Melt the DataFrame\n",
        "  melted_df = pd.melt(df, id_vars=['Digit Length'], var_name='NACE_DEPTH', value_name='Value')\n",
        "\n",
        "  # Pivot the melted DataFrame\n",
        "  pivot_df = pd.pivot_table(melted_df, index='NACE_DEPTH', columns='Digit Length', values='Value')\n",
        "\n",
        "  # Reset index to have 'Metric' as a column again\n",
        "  pivot_df.reset_index(inplace=True)\n",
        "\n",
        "  # Set column name index name to None\n",
        "  pivot_df.columns.name = None\n",
        "  existing_data = pd.read_excel(results_file)\n",
        "\n",
        "  new_df = pd.concat([pivot_df, settings_df], ignore_index=True)\n",
        "\n",
        "  invalid_char = [':','\\\\','?','*','[',']']\n",
        "  sheet_name = ''.join(char for char in settings_string if char not in invalid_char)\n",
        "  with pd.ExcelWriter(results_file, engine='openpyxl', mode='a') as writer:\n",
        "      for sheet_name, data in existing_data.items():\n",
        "          data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "      new_df.to_excel(writer, sheet_name=f'{sheet_name}', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # All settings we want to test for raportation\n",
        "    base_folder = '/content/drive/MyDrive/Thesis/models/Logistic_regression/models_test_code_final/'\n",
        "    results_folder = '/content/drive/MyDrive/Thesis/models/Logistic_regression/results_test_code_final/'\n",
        "    results_file = '/content/drive/MyDrive/Thesis/models/Logistic_regression/results_test_code_final/garbage.xlsx'\n",
        "    # List where all experiment settings will be saved to pass to main function\n",
        "    experiment_selections = list()\n",
        "    # 6660 for downsampling data based on first number, 100000000 - no downsampling\n",
        "    max_instances_options = [6660, 100000000] #6660 - 100000000\n",
        "    random_state_values = [32] # keep\n",
        "    tf_idf_max_features_options = [50000] # keep for all\n",
        "    iterations_max_options = [1000] # all models converge long before 1000 iteration\n",
        "    use_class_weighting_options = [True, False] # use class weighting?\n",
        "    use_embeddings_options = [True, False] # use embeddings options\n",
        "    pre_selected_use_directory_options = range(1, 10) # From which pre processed directory (pre made for faster processing during large scale testing)\n",
        "    experiment_selections = [base_folder, results_folder, results_file, max_instances_options,\n",
        "                             random_state_values, tf_idf_max_features_options, iterations_max_options,\n",
        "                            use_class_weighting_options, use_embeddings_options, pre_selected_use_directory_options]\n",
        "    main(experiment_selections)"
      ],
      "metadata": {
        "id": "sHMfnaq1LbpB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}