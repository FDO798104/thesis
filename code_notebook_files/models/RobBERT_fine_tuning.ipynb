{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSSNrrGsBO9C"
      },
      "outputs": [],
      "source": [
        "pip install transformers[torch] #RUN PIP command first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6SwAHdG7Sqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4668be-f79d-4ccf-9ced-933195d8ce18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoConfig, RobertaForSequenceClassification\n",
        "torch.manual_seed(36)\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(directory):\n",
        "    \"\"\"\n",
        "    Load data from parquet files in the specified directory.\n",
        "\n",
        "    Args:\n",
        "    - directory (str): The directory path where the parquet files are located.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Concatenated DataFrame containing data from all parquet files.\n",
        "    \"\"\"\n",
        "    data_files = os.listdir(directory)\n",
        "    dataframes = []\n",
        "\n",
        "    for file in data_files:\n",
        "        if file.endswith('.parquet'):\n",
        "            file_path = os.path.join(directory, file)\n",
        "            print(f\"Loading file {file_path}\")\n",
        "            dataframes.append(pd.read_parquet(file_path, columns=['Text', 'NACE']))\n",
        "\n",
        "    df = pd.concat(dataframes)\n",
        "    return df\n",
        "\n",
        "def downsample_data(df, time_median):\n",
        "    \"\"\"\n",
        "    Downsample the data based on the median number of samples per NACE category.\n",
        "\n",
        "    Args:\n",
        "    - df (pd.DataFrame): Input DataFrame containing the data.\n",
        "    - time_median (int): Median time value used for downsampling.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Downsampled DataFrame.\n",
        "    \"\"\"\n",
        "    resampled_dfs = []\n",
        "\n",
        "    median_samples = time_median * int(df['NACE'].value_counts().median())\n",
        "\n",
        "    def downsample_group(group):\n",
        "        if len(group) > median_samples:\n",
        "            return resample(group, replace=False, n_samples=median_samples, random_state=36)\n",
        "        return group\n",
        "\n",
        "    # Apply the downsample function to each group\n",
        "    resampled_df = df.groupby('NACE').apply(downsample_group).reset_index(drop=True)\n",
        "\n",
        "    print(resampled_df['NACE'].value_counts())\n",
        "    print(resampled_df)\n",
        "\n",
        "    return resampled_df"
      ],
      "metadata": {
        "id": "nxkKrJgmjvkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag045XRQzUj5"
      },
      "source": [
        "#Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDpWBPLT73vJ"
      },
      "outputs": [],
      "source": [
        "def split_data(df):\n",
        "    \"\"\"\n",
        "    Split the dataset into training, validation, and testing sets.\n",
        "\n",
        "    Args:\n",
        "    - df (pd.DataFrame): Input DataFrame containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Four data arrays: train_texts, test_texts, val_texts, train_labels, test_labels, val_labels.\n",
        "    \"\"\"\n",
        "    x = df['Text']\n",
        "    y = df['NACE']\n",
        "\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(x, y, test_size=0.3, random_state=36)\n",
        "    test_texts, val_texts, test_labels, val_labels = train_test_split(test_texts, test_labels, test_size=0.5, random_state=36)\n",
        "\n",
        "    # If the size of the training set is one more than a multiple of the batch size,\n",
        "    # the last row will be dropped to prevent an error during training the last batch.\n",
        "    if len(train_texts) % 8 == 1:\n",
        "        train_texts = train_texts[:-1]\n",
        "        train_labels = train_labels[:-1]\n",
        "        print(\"Dropped one row because of rest after division\")\n",
        "\n",
        "    return x, y, train_texts, test_texts, val_texts, train_labels, test_labels, val_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbuIFixSnR5f"
      },
      "outputs": [],
      "source": [
        "def calculate_num_labels(df):\n",
        "    \"\"\"\n",
        "    Calculate the number of unique labels in the 'NACE' column of the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - df (pd.DataFrame): Input DataFrame containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - int: Number of unique labels.\n",
        "    \"\"\"\n",
        "    num_labels = len(df['NACE'].unique())\n",
        "    return num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qafv7T7G9PaU"
      },
      "outputs": [],
      "source": [
        "def tokenize_texts(texts, tokenizer, truncation=True, padding=True):\n",
        "    \"\"\"\n",
        "    Tokenize a list of texts using the provided tokenizer.\n",
        "\n",
        "    Args:\n",
        "    - texts (list of str): List of texts to tokenize.\n",
        "    - tokenizer: Tokenizer object from Hugging Face's transformers library.\n",
        "    - truncation (bool): Whether to truncate the texts to the maximum length.\n",
        "    - padding (bool): Whether to pad the sequences to the maximum length.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Tokenized encodings.\n",
        "    \"\"\"\n",
        "    encodings = tokenizer(list(texts), truncation=truncation, padding=padding)\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr-2xDijhjXi"
      },
      "outputs": [],
      "source": [
        "def encode_labels(labels, label2id):\n",
        "    \"\"\"\n",
        "    Encode a list of labels using a label-to-id mapping.\n",
        "\n",
        "    Args:\n",
        "    - labels (list of str): List of labels to encode.\n",
        "    - label2id (dict): Mapping from labels to ids.\n",
        "\n",
        "    Returns:\n",
        "    - list of int: Encoded labels.\n",
        "    \"\"\"\n",
        "    labels_ids = [label2id[label] for label in labels]\n",
        "    return labels_ids\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EORMV6cf-m-v"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def create_dataset(encodings, labels):\n",
        "    \"\"\"\n",
        "    Create a PyTorch Dataset object.\n",
        "\n",
        "    Args:\n",
        "    - encodings (dict): Encodings of the dataset.\n",
        "    - labels (list): Labels of the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - Dataset: PyTorch Dataset object.\n",
        "    \"\"\"\n",
        "    dataset = Dataset(encodings, labels)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCfoEwgqzaaE"
      },
      "source": [
        "# New model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOrH5X5MtKae"
      },
      "outputs": [],
      "source": [
        "class CustomRobertaConfig(AutoConfig):\n",
        "    \"\"\"\n",
        "    Custom configuration class for a RoBERTa-based model with additional features.\n",
        "\n",
        "    Args:\n",
        "    - out_features (int): Number of output features for the custom classification head.\n",
        "    - **kwargs: Additional keyword arguments for the base configuration.\n",
        "\n",
        "    Attributes:\n",
        "    - out_features (int): Number of output features for the custom classification head.\n",
        "\n",
        "    Methods:\n",
        "    - to_dict(): Serialize the configuration instance to a Python dictionary.\n",
        "    - from_dict(config_dict): Instantiate a configuration from a Python dictionary.\n",
        "    \"\"\"\n",
        "    def __init__(self, out_features, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serialize this instance to a Python dictionary.\n",
        "        This implementation adds 'out_features' to the dictionary.\n",
        "        \"\"\"\n",
        "        config_dict = super().to_dict()\n",
        "        config_dict['out_features'] = self.out_features\n",
        "        return config_dict\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict):\n",
        "        \"\"\"\n",
        "        Instantiate a configuration from a Python dictionary of parameters.\n",
        "        This implementation loads 'out_features' from the dictionary.\n",
        "        \"\"\"\n",
        "        out_features = config_dict.pop('out_features', None)\n",
        "        return super().from_dict(config_dict, out_features=out_features)\n",
        "\n",
        "class CustomRobertaClassificationHead(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Custom classification head for a RoBERTa-based model with additional layers.\n",
        "\n",
        "    Args:\n",
        "    - config: Instance of CustomRobertaConfig containing model configuration.\n",
        "\n",
        "    Attributes:\n",
        "    - dense (torch.nn.Linear): Dense layer for feature transformation.\n",
        "    - dropout (torch.nn.Dropout): Dropout layer for regularization.\n",
        "    - batchnorm1 (torch.nn.BatchNorm1d): Batch normalization layer.\n",
        "    - out_proj (torch.nn.Linear): Output layer for classification.\n",
        "\n",
        "    Methods:\n",
        "    - forward(features, **kwargs): Forward pass through the classification head.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = torch.nn.Linear(config.hidden_size, config.out_features)\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.batchnorm1 = torch.nn.BatchNorm1d(config.out_features)\n",
        "        self.out_proj = torch.nn.Linear(config.out_features, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass through the classification head.\n",
        "\n",
        "        Args:\n",
        "        - features (torch.Tensor): Input features from the RoBERTa model.\n",
        "        - **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output logits for sequence classification.\n",
        "        \"\"\"\n",
        "        x = features[:, 0, :]  # Extract the first token's embeddings (CLS token)\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = nn.GELU()(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class CustomRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
        "    \"\"\"\n",
        "    Custom RoBERTa model for sequence classification with a custom classification head.\n",
        "\n",
        "    Args:\n",
        "    - config: Instance of CustomRobertaConfig containing model configuration.\n",
        "\n",
        "    Attributes:\n",
        "    - classifier (CustomRobertaClassificationHead): Custom classification head.\n",
        "\n",
        "    Methods:\n",
        "    - forward(**kwargs): Forward pass through the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.classifier = CustomRobertaClassificationHead(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLvBhffKOe0t"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics for a classification task.\n",
        "\n",
        "    Args:\n",
        "    - eval_pred (tuple): Tuple containing predictions and labels.\n",
        "        - predictions (numpy.ndarray): Predicted probabilities or logits.\n",
        "        - labels (numpy.ndarray): True labels (one-hot encoded or class indices).\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary containing computed metrics.\n",
        "        - \"accuracy\" (float): Accuracy of the predictions.\n",
        "        - \"f1\" (float): Weighted F1-score of the predictions.\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, predictions.argmax(axis=1))\n",
        "\n",
        "    # Calculate F1-score\n",
        "    f1 = f1_score(labels, predictions.argmax(axis=1), average='weighted')\n",
        "\n",
        "    # You can calculate other metrics like precision, recall, F1-score, etc.\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzNCa4aTAQQs"
      },
      "outputs": [],
      "source": [
        "def create_trainer(model, train_dataset, val_dataset, output_dir, logging_dir, num_train_epochs, freeze_bottom_layers=False):\n",
        "    \"\"\"\n",
        "    Train the provided model using the specified datasets and configuration.\n",
        "\n",
        "    Args:\n",
        "    - model: The model to train.\n",
        "    - train_dataset: Dataset object for training data.\n",
        "    - val_dataset: Dataset object for validation data.\n",
        "    - output_dir (str): Directory to save the model checkpoints.\n",
        "    - logging_dir (str): Directory for storing training logs.\n",
        "    - num_train_epochs (int): Number of training epochs.\n",
        "    - freeze_bottom_layers (bool): Whether to freeze the parameters of the bottom layers.\n",
        "\n",
        "    Returns:\n",
        "    - Trainer: Trainer object for training the model.\n",
        "    \"\"\"\n",
        "    # Ensure directories exist\n",
        "    if not os.path.isdir(output_dir):\n",
        "        raise FileNotFoundError(f\"Output directory '{output_dir}' not found.\")\n",
        "    if not os.path.isdir(logging_dir):\n",
        "        raise FileNotFoundError(f\"Logging directory '{logging_dir}' not found.\")\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=logging_dir,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    # Freeze bottom layers if specified\n",
        "    if freeze_bottom_layers:\n",
        "        for name, param in model.named_parameters():\n",
        "            if name.startswith('roberta.encoder.layer') and int(name.split('.')[3]) < 5:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Trainer initialization\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c6xa1EJRLyK"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iATMxZnWcIHm"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics for a classification task.\n",
        "\n",
        "    Args:\n",
        "    - eval_pred (tuple): Tuple containing predictions and labels.\n",
        "        - predictions (numpy.ndarray): Predicted probabilities or logits.\n",
        "        - labels (numpy.ndarray): True labels (one-hot encoded or class indices).\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary containing computed metrics.\n",
        "        - \"accuracy\" (float): Accuracy of the predictions.\n",
        "        - \"f1\" (float): Weighted F1-score of the predictions.\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, predictions.argmax(axis=1))\n",
        "\n",
        "    # Calculate F1-score\n",
        "    f1 = f1_score(labels, predictions.argmax(axis=1), average='weighted')\n",
        "\n",
        "    # You can calculate other metrics like precision, recall, F1-score, etc.\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGwfwkKG9aV2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoConfig, RobertaForSequenceClassification\n",
        "\n",
        "class CustomRobertaConfig(AutoConfig):\n",
        "    \"\"\"\n",
        "    Custom configuration class for a RoBERTa-based model with additional features.\n",
        "\n",
        "    Args:\n",
        "    - out_features (int): Number of output features for the custom classification head.\n",
        "    - **kwargs: Additional keyword arguments for the base configuration.\n",
        "\n",
        "    Attributes:\n",
        "    - out_features (int): Number of output features for the custom classification head.\n",
        "\n",
        "    Methods:\n",
        "    - to_dict(): Serialize the configuration instance to a Python dictionary.\n",
        "    - from_dict(config_dict): Instantiate a configuration from a Python dictionary.\n",
        "    \"\"\"\n",
        "    def __init__(self, out_features, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Serialize this instance to a Python dictionary.\n",
        "        This implementation adds 'out_features' to the dictionary.\n",
        "        \"\"\"\n",
        "        config_dict = super().to_dict()\n",
        "        config_dict['out_features'] = self.out_features\n",
        "        return config_dict\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict):\n",
        "        \"\"\"\n",
        "        Instantiate a configuration from a Python dictionary of parameters.\n",
        "        This implementation loads 'out_features' from the dictionary.\n",
        "        \"\"\"\n",
        "        out_features = config_dict.pop('out_features', None)\n",
        "        return super().from_dict(config_dict, out_features=out_features)\n",
        "\n",
        "class CustomRobertaClassificationHead(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Custom classification head for a RoBERTa-based model with additional layers.\n",
        "\n",
        "    Args:\n",
        "    - config: Instance of CustomRobertaConfig containing model configuration.\n",
        "\n",
        "    Attributes:\n",
        "    - dense (torch.nn.Linear): Dense layer for feature transformation.\n",
        "    - dropout (torch.nn.Dropout): Dropout layer for regularization.\n",
        "    - batchnorm1 (torch.nn.BatchNorm1d): Batch normalization layer.\n",
        "    - out_proj (torch.nn.Linear): Output layer for classification.\n",
        "\n",
        "    Methods:\n",
        "    - forward(features, **kwargs): Forward pass through the classification head.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = torch.nn.Linear(config.hidden_size, config.out_features)\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.batchnorm1 = torch.nn.BatchNorm1d(config.out_features)\n",
        "        self.out_proj = torch.nn.Linear(config.out_features, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass through the classification head.\n",
        "\n",
        "        Args:\n",
        "        - features (torch.Tensor): Input features from the RoBERTa model.\n",
        "        - **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output logits for sequence classification.\n",
        "        \"\"\"\n",
        "        x = features[:, 0, :]  # Extract the first token's embeddings (CLS token)\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = nn.GELU()(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class CustomRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
        "    \"\"\"\n",
        "    Custom RoBERTa model for sequence classification with a custom classification head.\n",
        "\n",
        "    Args:\n",
        "    - config: Instance of CustomRobertaConfig containing model configuration.\n",
        "\n",
        "    Attributes:\n",
        "    - classifier (CustomRobertaClassificationHead): Custom classification head.\n",
        "\n",
        "    Methods:\n",
        "    - forward(**kwargs): Forward pass through the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.classifier = CustomRobertaClassificationHead(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUVMsn_eQ_uU"
      },
      "outputs": [],
      "source": [
        "def trainer_for_evaluation(model_dir, output_dir, val_dataset, compute_metrics_func, freeze_bottom_layers=False):\n",
        "    \"\"\"\n",
        "    Reload a trained RoBERTa model from a checkpoint and prepare it for evaluation.\n",
        "\n",
        "    Args:\n",
        "    - model_dir (str): Directory path for the existing model.\n",
        "    - val_dataset: Dataset object for validation data.\n",
        "    - compute_metrics_func (function): Function for computing evaluation metrics.\n",
        "    - freeze_bottom_layers (bool): Whether to freeze the parameters of the bottom layers.\n",
        "\n",
        "    Returns:\n",
        "    - Trainer: Trainer object for evaluation.\n",
        "    \"\"\"\n",
        "    # Reload the trained model\n",
        "    user_input = input(\"MAKE SURE YOU CHECK THE DIRECTORIES BEFORE RUNNING! Continue (yes/no)?\")\n",
        "    if user_input != 'yes':\n",
        "        raise ValueError('User did not respond with \\'yes\\'.')\n",
        "\n",
        "    # Load the configuration\n",
        "    config = CustomRobertaConfig.from_pretrained(model_dir)\n",
        "    # Load the model\n",
        "    model = CustomRobertaForSequenceClassification.from_pretrained(model_dir, config=config)\n",
        "    print(model)\n",
        "\n",
        "    # Freeze bottom layers if specified\n",
        "    if freeze_bottom_layers:\n",
        "        for name, param in model.named_parameters():\n",
        "            if name.startswith('roberta.encoder.layer') and int(name.split('.')[3]) < 5:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Initialize Trainer for evaluation\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(output_dir=output_dir),\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics_func\n",
        "\n",
        "    )\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsClMWlt-fC7"
      },
      "source": [
        "##Evaluation on nr of digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWdK4dut-OFI"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(original_nace_codes, pred_nace_codes, level):\n",
        "  original_nace_level = [code[:level] for code in original_nace_codes]\n",
        "  pred_nace_level = [code[:level] for code in pred_nace_codes]\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = accuracy_score(original_nace_level, pred_nace_level)\n",
        "\n",
        "  # Calculate F1-score\n",
        "  f1 = f1_score(original_nace_level, pred_nace_level, average='weighted')\n",
        "\n",
        "  return accuracy, f1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_nace_codes(nace_labels):\n",
        "    \"\"\"\n",
        "    Corrects NACE codes by adding leading zeros to labels with 4 digits.\n",
        "\n",
        "    Args:\n",
        "    - nace_labels (list of str): List of NACE codes to be corrected.\n",
        "\n",
        "    Returns:\n",
        "    - list of str: List of corrected NACE codes.\n",
        "    \"\"\"\n",
        "    corrected_labels = []\n",
        "    for label in nace_labels:\n",
        "        if len(label) == 4 and label.isdigit():\n",
        "            label = \"0\" + label\n",
        "        corrected_labels.append(label)\n",
        "    return corrected_labels"
      ],
      "metadata": {
        "id": "Fj6EPGYkoIox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srz4JxWh-RfI"
      },
      "outputs": [],
      "source": [
        "def evaluate_nace_code_classification(labels_corrected, pred_labels_corrected):\n",
        "    \"\"\"\n",
        "    Evaluate NACE code classification performance at different levels of classification.\n",
        "\n",
        "    Args:\n",
        "    - val_labels_corrected (list of str): List of corrected original NACE codes.\n",
        "    - val_pred_labels_corrected (list of str): List of corrected predicted NACE codes.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary containing accuracy and F1-score for each level of classification.\n",
        "    \"\"\"\n",
        "    evaluation_results = {}\n",
        "    for level in range(1, 6):\n",
        "        accuracy, f1 = calculate_metrics(labels_corrected, pred_labels_corrected, level)\n",
        "        evaluation_results[f\"{level} digits\"] = {\"Accuracy\": accuracy, \"F1-score\": f1}\n",
        "    return evaluation_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "A2IF0LjrJ2x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the entire pipeline of loading, preprocessing, training, and evaluating the model.\n",
        "    \"\"\"\n",
        "    # Directory containing the data\n",
        "    directory = '/content/drive/...'\n",
        "\n",
        "    # Median time value for downsampling\n",
        "    time_median = 1\n",
        "\n",
        "    # Load the data\n",
        "    df = load_data(directory)\n",
        "\n",
        "    # Downsample the data\n",
        "    df_downsampled = downsample_data(df, time_median)\n",
        "\n",
        "    # Split the dataset into training, validation, and testing sets\n",
        "    x, y, train_texts, test_texts, val_texts, train_labels, test_labels, val_labels = split_data(df)\n",
        "\n",
        "    # Calculate the number of unique labels\n",
        "    num_labels = calculate_num_labels(df)\n",
        "\n",
        "    # Tokenize the texts\n",
        "    tokenizer = AutoTokenizer.from_pretrained('DTAI-KULeuven/robbert-2023-dutch-base')\n",
        "    train_encodings = tokenize_texts(train_texts, tokenizer)\n",
        "    val_encodings = tokenize_texts(val_texts, tokenizer)\n",
        "    test_encodings = tokenize_texts(test_texts, tokenizer)\n",
        "\n",
        "    # Encode the labels\n",
        "    label2id = {label: id for id, label in enumerate(sorted(set(y)))}\n",
        "    inverse_label_map = {id: label for label, id in label2id.items()}\n",
        "\n",
        "    train_labels_ids = encode_labels(train_labels, label2id)\n",
        "    val_labels_ids = encode_labels(val_labels, label2id)\n",
        "    test_labels_ids = encode_labels(test_labels, label2id)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = create_dataset(train_encodings, train_labels_ids)\n",
        "    val_dataset = create_dataset(val_encodings, val_labels_ids)\n",
        "    test_dataset = create_dataset(test_encodings, test_labels_ids)\n",
        "\n",
        "    # Load the configuration\n",
        "    config = CustomRobertaConfig.from_pretrained('DTAI-KULeuven/robbert-2023-dutch-base')\n",
        "    # Modify the configuration as needed\n",
        "    config.num_labels = num_labels  # Change the number of labels if necessary\n",
        "    config.out_features = 4096  # Modify the out_features as desired\n",
        "\n",
        "    # Load the model\n",
        "    model = CustomRobertaForSequenceClassification.from_pretrained('DTAI-KULeuven/robbert-2023-dutch-base', config=config)\n",
        "    print(model)\n",
        "\n",
        "    # Specify output and logging directories\n",
        "    output_dir = '/content/drive/...'\n",
        "    logging_dir = '/content/drive/...'\n",
        "\n",
        "    # Ensure user confirmation before continuing\n",
        "    user_input = input(\"MAKE SURE YOU CHECK THE DIRECTORIES BEFORE RUNNING! Continue (yes/no)?\")\n",
        "    if user_input != 'yes':\n",
        "        raise ValueError('User did not respond with \\'yes\\'.')\n",
        "\n",
        "    # Check if required directory exists\n",
        "    if not os.path.isdir('/content/drive/...'):\n",
        "        raise FileNotFoundError('Folder not found')\n",
        "\n",
        "    # Create trainer for training\n",
        "    trainer = create_trainer(model, train_dataset, val_dataset, output_dir, logging_dir, num_train_epochs=1, freeze_bottom_layers=True)\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Specify model checkpoint directory\n",
        "    model_dir = '/content/drive/...'\n",
        "\n",
        "    # Create trainer for evaluation\n",
        "    trainer = trainer_for_evaluation(model_dir, output_dir, val_dataset, compute_metrics, freeze_bottom_layers=True)\n",
        "\n",
        "    # Make predictions on validation dataset\n",
        "    val_predictions = trainer.predict(val_dataset)\n",
        "    val_pred_label_ids = val_predictions.predictions.argmax(axis=1).tolist()\n",
        "    val_pred_labels = [inverse_label_map[label_id] for label_id in val_pred_label_ids]\n",
        "\n",
        "    # Correct predicted NACE codes if necessary\n",
        "    val_pred_labels_corrected = correct_nace_codes(val_pred_labels)\n",
        "    val_labels_corrected = correct_nace_codes(val_labels.to_list())\n",
        "\n",
        "    # Evaluate NACE code classification\n",
        "    val_evaluation_results = evaluate_nace_code_classification(val_labels_corrected, val_pred_labels_corrected)\n",
        "    for level, metrics in val_evaluation_results.items():\n",
        "        print(f\"{level}: Accuracy = {metrics['Accuracy']}; F1 = {metrics['F1-score']}\")\n",
        "\n",
        "    # Make predictions on test dataset\n",
        "    test_predictions = trainer.predict(test_dataset)\n",
        "    test_pred_label_ids = test_predictions.predictions.argmax(axis=1).tolist()\n",
        "    test_pred_labels = [inverse_label_map[label_id] for label_id in test_pred_label_ids]\n",
        "\n",
        "    # Correct predicted NACE codes if necessary\n",
        "    test_pred_labels_corrected = correct_nace_codes(test_pred_labels)\n",
        "    test_labels_corrected = correct_nace_codes(test_labels.to_list())\n",
        "\n",
        "    # Evaluate NACE code classification\n",
        "    test_evaluation_results = evaluate_nace_code_classification(test_labels_corrected, test_pred_labels_corrected)\n",
        "    for level, metrics in test_evaluation_results.items():\n",
        "        print(f\"{level}: Accuracy = {metrics['Accuracy']}; F1 = {metrics['F1-score']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "MQ73JCPwhCDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WCfoEwgqzaaE"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}